{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Envrionment: UQ_qoi_bugfix\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import fsspec\n",
    "import pvlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from eulpuq.qoi.quantities_of_interest import QuantityOfInterest\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from Base import Base as Base\n",
    "\n",
    "class ComStock_Surrogate():\n",
    "    \"\"\"\n",
    "    __init__:\n",
    "        version: now only tested in 'com_reg1_02_01_01_2016'\n",
    "        result_csv: directory of result csv or parquet\n",
    "        raw_more_building_characteristics: directory of result csv with extra building characteristics\n",
    "        read_data_from_S3: True: read data from S3 and save only relevant data locally; False: read locally saved data\n",
    "        number_of_trees: number of trees in the random forest algorithm\n",
    "        building_type: the string of one building type\n",
    "        result_csv_for_prediction: directory of result csv or parquet for prediction\n",
    "        raw_more_building_characteristics_for_prediction: directory of result csv with extra building characteristics or prediction\n",
    "        enduse_list: enduse of ComStock output, e.g. ['total_site_electricity_kwh']. Now only support single element list\n",
    "        more_building_characteritics_list: a list of building characteristics in \"raw_more_building_characteristics\" to be added\n",
    "        validation: whether to do the validation during training process\n",
    "        root_path: the root path for running the code and for generating data and plots\n",
    "\n",
    "    functions:\n",
    "        create_folder_structure: create folder structure for data and results\n",
    "        inputs_generator: generate inputs based on result csv and weather data\n",
    "        output_generator: generate output from simulation results\n",
    "        get_models: train or load trained machine learning models\n",
    "        make_predictions: make predictions based on inputs for testing and model\n",
    "        QOI_calculators: calculate QOIs based on Siby's QOI module\n",
    "        whole_process_only_testing: whole process for testing\n",
    "        whole_process_only_training: whole process for training\n",
    "        whole_process_training_and_testingL whole process for both training and testing\n",
    "\n",
    "    \"\"\"\n",
    "    # Finalize the __init___'s variables\n",
    "    def __init__(self, version, result_csv, raw_more_building_characteristics, read_data_from_S3, number_of_trees,\n",
    "                 building_type, more_building_characteritics_list = Base.more_building_characteritics_list,\n",
    "                 root_path = os.getcwd(), validation = True, result_csv_for_prediction = 'none',\n",
    "                 raw_more_building_characteristics_for_prediction = 'none',\n",
    "                 enduse_list = ['total_site_electricity_kwh']):\n",
    "        self.version = version\n",
    "        self.result_csv = Base.process_raw_result_parquet_file_to_extract_basic_building_characteristics(result_csv)\n",
    "        self.raw_more_building_characteristics = pd.read_csv(raw_more_building_characteristics).sort_values(by=['building_id'], ascending = True)\n",
    "        self.read_data_from_S3 = read_data_from_S3\n",
    "        self.number_of_trees = number_of_trees\n",
    "        self.building_type = building_type\n",
    "        self.result_csv_for_prediction = Base.process_raw_result_parquet_file_to_extract_basic_building_characteristics(result_csv_for_prediction)\n",
    "        self.raw_more_building_characteristics_for_prediction = pd.read_csv(raw_more_building_characteristics_for_prediction).sort_values(by=['building_id'], ascending = True)\n",
    "        self.enduse_list = enduse_list\n",
    "        self.more_building_characteritics_list = more_building_characteritics_list\n",
    "        self.validation = validation\n",
    "        self.root_path = root_path\n",
    "\n",
    "    def create_folder_structure(self):\n",
    "        print('Creating folder structure...')\n",
    "        folders = [f'data/{self.version}',f'models/{self.version}',f'plots/{self.version}',\n",
    "                   f'results/{self.version}', f'temp/{self.version}', f'weather/{self.version}']\n",
    "        for folder in folders:\n",
    "            if not os.path.exists(os.path.join(self.root_path, folder)):\n",
    "                os.makedirs(os.path.join(self.root_path, folder))\n",
    "\n",
    "    def inputs_generator(self, train_or_test):\n",
    "        print(f'Generating inputs for {train_or_test}ing...')\n",
    "        if train_or_test == 'train':\n",
    "            result_csv = self.result_csv\n",
    "            raw_more_building_characteristics = self.raw_more_building_characteristics\n",
    "        elif train_or_test == 'test':\n",
    "            result_csv = self.result_csv_for_prediction\n",
    "            raw_more_building_characteristics = self.raw_more_building_characteristics_for_prediction\n",
    "        else:\n",
    "            print(\"Error! Enter either 'train' or 'test' for train_or_test\")\n",
    "\n",
    "        enduse_list = self.enduse_list\n",
    "        building_type = self.building_type\n",
    "        more_building_characteritics_list = self.more_building_characteritics_list\n",
    "\n",
    "        # time indicator\n",
    "        building_ID_list_one_building_type = result_csv.loc[result_csv.building_type == building_type].building_id.unique()\n",
    "        time_df = pd.DataFrame([])\n",
    "        date_range = pd.date_range(\"01-01-2016 00:00\", \"12-30-2016 23:00\", freq=\"60min\")\n",
    "        time_df['hour'] = date_range.hour\n",
    "        time_df['month'] = date_range.month\n",
    "        time_df['weekday'] = date_range.weekday\n",
    "        time_df['weekday_indicator'] = [int(x>=5) for x in time_df['weekday']]\n",
    "        time_df = pd.concat([time_df] * len(building_ID_list_one_building_type)).reset_index(drop = True)\n",
    "        time_df['building_id'] = building_ID_list_one_building_type.repeat(24*365)\n",
    "        time_df = time_df.reindex(columns=['building_id','hour','month','weekday','weekday_indicator'])\n",
    "        # weather\n",
    "        weather_df = Base.get_weather_df(weather_file_location = f'weather/{self.version}/',\n",
    "                       weather_filename = 'USA_CO_Fort.Collins.Sawrs.724697_2016.epw',\n",
    "                       cols = ['temp_air', 'temp_dew', 'relative_humidity', 'atmospheric_pressure',\n",
    "                               'etr', 'etrn','ghi_infrared','ghi','dni','dhi','global_hor_illum',\n",
    "                               'direct_normal_illum', 'diffuse_horizontal_illum', 'zenith_luminance',\n",
    "                               'wind_direction', 'wind_speed'],\n",
    "                       timelag = 2\n",
    "                      )\n",
    "        weather_df = weather_df[0:8760]\n",
    "        weather_df = pd.concat([weather_df]* len(building_ID_list_one_building_type)).reset_index(drop = True)\n",
    "        # building characteristics\n",
    "        building_characteristics = result_csv.loc[result_csv.building_id.isin(building_ID_list_one_building_type)]\n",
    "        building_characteristics = building_characteristics.iloc[\n",
    "            np.repeat(np.arange(len(building_characteristics)), 24*365)].reset_index(drop=True)\n",
    "        # more building characteristics including operation status\n",
    "        extracted_df = raw_more_building_characteristics[more_building_characteritics_list]\n",
    "        extracted_df = extracted_df.loc[extracted_df.building_id.isin(building_ID_list_one_building_type)]\n",
    "        new_extracted_df = pd.DataFrame(np.repeat(extracted_df.values,24*365,axis=0))\n",
    "        new_extracted_df.columns = extracted_df.columns\n",
    "        new_extracted_df = new_extracted_df.drop(columns= ['building_id'])\n",
    "        new_extracted_df = new_extracted_df.astype(float)\n",
    "        # operation status\n",
    "        time_df_temp = pd.DataFrame([])\n",
    "        date_range_temp = pd.date_range(\"01-01-2016 00:00\", \"12-30-2016 23:00\", freq=\"1440min\")\n",
    "        time_df_temp['weekday'] = date_range_temp.weekday\n",
    "        time_df_temp['weekday_indicator'] = [int(x>=5) for x in time_df_temp['weekday']]\n",
    "        time_df_indicator = time_df_temp.weekday_indicator\n",
    "        full_schedule_list = []\n",
    "        for building_id in building_ID_list_one_building_type:\n",
    "            temp = result_csv.loc[result_csv.building_id == building_id]\n",
    "            weekday_start_time = temp['build_existing_model.create_typical_building_from_model_wkdy_op_hrs_start_time'].values[0]\n",
    "            weekday_duration_time = temp ['build_existing_model.create_typical_building_from_model_wkdy_op_hrs_duration'].values[0]\n",
    "            weekend_start_time = temp ['build_existing_model.create_typical_building_from_model_wknd_op_hrs_start_time'].values[0]\n",
    "            weekend_duration_time = temp ['build_existing_model.create_typical_building_from_model_wknd_op_hrs_duration'].values[0]\n",
    "            weekday_stop_time = weekday_duration_time + weekday_start_time\n",
    "            weekend_stop_time = weekend_duration_time + weekend_start_time\n",
    "            for x in time_df_indicator:\n",
    "                if x == 0:\n",
    "                    full_schedule_list += [int(x>=weekday_start_time and x<=weekday_stop_time) for x in range(24)]\n",
    "                else:\n",
    "                    full_schedule_list += [int(x>=weekend_start_time and x<=weekend_stop_time) for x in range(24)]\n",
    "        new_extracted_df['operation_status'] = full_schedule_list\n",
    "\n",
    "        inputs = pd.concat([building_characteristics, weather_df, time_df, new_extracted_df], axis = 1)\n",
    "        # drop duplicated columns: building_id\n",
    "        inputs = inputs.loc[:,~inputs.columns.duplicated()]#.iloc[:,2:]\n",
    "        inputs = dd.from_pandas(inputs, npartitions=10)\n",
    "\n",
    "        if train_or_test == 'train':\n",
    "            self.inputs_train = inputs\n",
    "        elif train_or_test == 'test':\n",
    "            self.inputs_test = inputs\n",
    "        else:\n",
    "            print(\"Error! Enter either 'train' or 'test' for train_or_test\")\n",
    "\n",
    "    def output_generator(self):\n",
    "        print('Generating output...')\n",
    "        read_data_from_S3 = self.read_data_from_S3\n",
    "        version = self.version\n",
    "        result_csv = self.result_csv\n",
    "        enduse_list = self.enduse_list\n",
    "        building_type = self.building_type\n",
    "\n",
    "        if read_data_from_S3:\n",
    "            Base.get_ComStock_simulation_data_from_S3(version, enduse_list)\n",
    "        else:\n",
    "            # Read preread data\n",
    "            df_raw = pd.read_csv(f'data/{version}/df_raw.csv')\n",
    "        building_ID_list_one_building_type = result_csv.loc[result_csv.building_type == building_type].building_id.unique()\n",
    "        df_raw = df_raw.loc[df_raw.building_id.isin(building_ID_list_one_building_type)].reset_index(drop = True)\n",
    "\n",
    "        self.output_train_annual = dd.from_pandas(df_raw[enduse_list], npartitions=10)\n",
    "\n",
    "        self.output_train_annual_total = df_raw.groupby(df_raw.index // 8760).sum()\n",
    "        #self.output_train_annual_total = self.output_train_annual_total.iloc[np.repeat(np.arange(len(self.output_train_annual_total)), 8760)].reset_index(drop = True)\n",
    "        self.output_train_annual_total = dd.from_pandas(self.output_train_annual_total[enduse_list], npartitions=10)\n",
    "\n",
    "        #df_raw[enduse_list[0]] = df_raw[enduse_list[0]] / (self.inputs_train['build_existing_model.create_bar_from_building_type_ratios_total_bldg_floor_area']).compute()\n",
    "        #df_raw[enduse_list[0]] = df_raw[enduse_list[0]] / (self.inputs_train['build_existing_model.rentable_area']).compute()\n",
    "        temp_annual_energy = df_raw.groupby(df_raw.index // 8760).sum()\n",
    "        temp_annual_energy = temp_annual_energy.iloc[np.repeat(np.arange(len(temp_annual_energy)), 8760)].reset_index(drop = True)\n",
    "        df_raw[enduse_list[0]] = df_raw[enduse_list[0]] / temp_annual_energy[enduse_list[0]]\n",
    "\n",
    "        # output_shape = dd.from_pandas(df_raw[enduse_list], npartitions=10)\n",
    "        # self.output_train_shape = output_shape\n",
    "\n",
    "        self.output_train = dd.from_pandas(df_raw[enduse_list], npartitions=10)\n",
    "\n",
    "    def bad_building_classifier(self, train_or_load_model):\n",
    "        # version = self.version\n",
    "        # result_csv = self.result_csv\n",
    "        # building_type = self.building_type\n",
    "        pass\n",
    "\n",
    "\n",
    "    def get_models(self, train_or_load_model):\n",
    "        version = self.version\n",
    "        result_csv = self.result_csv\n",
    "        building_type = self.building_type\n",
    "        if train_or_load_model == 'train':\n",
    "            print(f'Training Model: {building_type} ...')\n",
    "            # Here we train the annual building energy consumption model\n",
    "            annual_load_model = RandomForestRegressor(n_estimators = 20, random_state=42)\n",
    "            annual_load_model.fit(self.inputs_train.iloc[:,2:].groupby(self.inputs_train.iloc[:,2:].index //8760).mean(), self.output_train_annual_total.iloc[:,0].values.ravel())\n",
    "\n",
    "            feature_importance_df_annual_energy = pd.DataFrame([])\n",
    "            feature_importance_df_annual_energy['feature_name'] = self.inputs_train.iloc[:,2:].columns\n",
    "            feature_importance_df_annual_energy['feature_importance'] = annual_load_model.feature_importances_\n",
    "            importance_mean = np.quantile(feature_importance_df_annual_energy.feature_importance, 0.5)\n",
    "            feature_importance_df_annual_energy = feature_importance_df_annual_energy.loc[feature_importance_df_annual_energy.feature_importance >= importance_mean].iloc[:,0].tolist()\n",
    "\n",
    "            self.feature_importance_df_annual_energy = feature_importance_df_annual_energy\n",
    "\n",
    "            annual_load_model = RandomForestRegressor(n_estimators = 20, random_state=42)\n",
    "            annual_load_model.fit(self.inputs_train[feature_importance_df_annual_energy].groupby(self.inputs_train[feature_importance_df_annual_energy].index //8760).mean(), self.output_train_annual_total.iloc[:,0].values.ravel())\n",
    "\n",
    "            # # Here we do a cross validation to decide the optimal features\n",
    "            kf = KFold(n_splits=5)\n",
    "            building_ID_list_one_building_type = result_csv.loc[result_csv.building_type == building_type].building_id.unique()\n",
    "            selected_feature_list = []\n",
    "\n",
    "            for train_index, test_index in kf.split(building_ID_list_one_building_type):\n",
    "                building_id_list_for_training = [building_ID_list_one_building_type[index] for index in train_index]\n",
    "\n",
    "                X_train = (self.inputs_train).loc[(self.inputs_train).building_id.isin(building_id_list_for_training)].iloc[:,2:]\n",
    "                y_train = (self.output_train).loc[(self.inputs_train).building_id.isin(building_id_list_for_training)]\n",
    "\n",
    "                regr = RandomForestRegressor(n_estimators = 5, random_state=42)\n",
    "                regr.fit(X_train, y_train.iloc[:,0].values.ravel())\n",
    "                feature_importance_df = pd.DataFrame([])\n",
    "                feature_importance_df['feature_name'] = X_train.columns\n",
    "                feature_importance_df['feature_importance'] = regr.feature_importances_\n",
    "                importance_mean = np.quantile(feature_importance_df.feature_importance, 0.75)\n",
    "                feature_importance_df = feature_importance_df.loc[feature_importance_df.feature_importance >= importance_mean]\n",
    "\n",
    "                feature_importance_df_final = pd.DataFrame([])\n",
    "                feature_importance_df_final[self.building_type] = feature_importance_df.sort_values(\n",
    "                    ['feature_importance'], ascending = False).feature_name.values\n",
    "                temp_feature_list = feature_importance_df_final[self.building_type].tolist()\n",
    "                selected_feature_list += temp_feature_list\n",
    "\n",
    "            selected_feature_list = list(set(selected_feature_list))\n",
    "            self.selected_feature_list = selected_feature_list\n",
    "\n",
    "            # Here we did the random split of building ID for training and testing\n",
    "            #building_ID_list_one_building_type = result_csv.loc[result_csv.building_type == building_type].building_id.unique()\n",
    "            train_building_id, test_building_id = train_test_split(building_ID_list_one_building_type,test_size=0.20, random_state=42)\n",
    "\n",
    "            # X_train = (self.inputs_train).loc[(self.inputs_train).building_id.isin(train_building_id)].iloc[:,2:]\n",
    "            # X_test = (self.inputs_train).loc[(self.inputs_train).building_id.isin(test_building_id)].iloc[:,2:]\n",
    "            X_train = (self.inputs_train).loc[(self.inputs_train).building_id.isin(train_building_id)][selected_feature_list]\n",
    "            X_test = (self.inputs_train).loc[(self.inputs_train).building_id.isin(test_building_id)][selected_feature_list]\n",
    "\n",
    "            y_train = (self.output_train).loc[(self.inputs_train).building_id.isin(train_building_id)]\n",
    "            y_test = (self.output_train).loc[(self.inputs_train).building_id.isin(test_building_id)]\n",
    "\n",
    "            y_train_annual = (self.output_train_annual).loc[(self.inputs_train).building_id.isin(train_building_id)]\n",
    "            y_test_annual = (self.output_train_annual).loc[(self.inputs_train).building_id.isin(test_building_id)]\n",
    "\n",
    "            regr = RandomForestRegressor(n_estimators = self.number_of_trees, random_state=42) #max_depth = 10,\n",
    "            # regr = Earth()\n",
    "            regr.fit(X_train, y_train.iloc[:,0].values.ravel())\n",
    "            self.model = regr\n",
    "            pickle.dump(self.model, open(f'models/{version}/{building_type}.sav', 'wb'))\n",
    "            print('Training Model Completed!')\n",
    "\n",
    "            if self.validation == True:\n",
    "                feature_importance_df_final = pd.DataFrame([])\n",
    "                error_df = pd.DataFrame([])\n",
    "\n",
    "                feature_importance_df = pd.DataFrame([])\n",
    "                feature_importance_df['feature_name'] = X_train.columns\n",
    "                feature_importance_df['feature_importance'] = regr.feature_importances_\n",
    "\n",
    "                feature_importance_df_final [self.building_type] = feature_importance_df.sort_values(\n",
    "                    ['feature_importance'], ascending = False).feature_name.values\n",
    "\n",
    "                # y_train_predicted = regr.predict(X_train)\n",
    "                # y_test_predicted = regr.predict(X_test)\n",
    "                temp_train = (self.inputs_train).loc[(self.inputs_train).building_id.isin(train_building_id)][feature_importance_df_annual_energy]\n",
    "                temp_test = (self.inputs_train).loc[(self.inputs_train).building_id.isin(test_building_id)][feature_importance_df_annual_energy]\n",
    "\n",
    "                temp_2_train = pd.DataFrame(annual_load_model.predict(temp_train.groupby(temp_train.index //8760).mean()))\n",
    "                temp_2_test = pd.DataFrame(annual_load_model.predict(temp_test.groupby(temp_test.index //8760).mean()))\n",
    "\n",
    "                # print(temp_2_train.iloc[np.repeat(np.arange(len(temp_2_train)), 8760)].reset_index(drop = True))\n",
    "                # print(len(temp_2_train.iloc[np.repeat(np.arange(len(temp_2_train)), 8760)].reset_index(drop = True)))\n",
    "                #\n",
    "                # print(regr.predict(X_train))\n",
    "                #\n",
    "                # print(type(regr.predict(X_train)))\n",
    "                # print(len(regr.predict(X_train)))\n",
    "                # print(regr.predict(X_train).shape)\n",
    "                #\n",
    "                # print(pd.Series(regr.predict(X_train)))\n",
    "                y_train_predicted = pd.Series(regr.predict(X_train)) * temp_2_train.iloc[np.repeat(np.arange(len(temp_2_train)), 8760)].reset_index(drop = True).iloc[:,0]\n",
    "                y_test_predicted = pd.Series(regr.predict(X_test)) * temp_2_test.iloc[np.repeat(np.arange(len(temp_2_test)), 8760)].reset_index(drop = True).iloc[:,0]\n",
    "                self.y_test_predicted = y_test_predicted\n",
    "\n",
    "                # y_train = y_train.compute()\n",
    "                # y_train_predicted = y_train_predicted\n",
    "                # y_test = y_test.compute()\n",
    "                # y_test_predicted = y_test_predicted\n",
    "                y_train = y_train_annual.compute()\n",
    "                #y_train_predicted = y_train_predicted\n",
    "                y_test = y_test_annual.compute()\n",
    "                self.y_test = y_test\n",
    "                #y_test_predicted = y_test_predicted\n",
    "\n",
    "                training_error = (mean_squared_error(y_train.values, y_train_predicted))**0.5/y_train.mean()\n",
    "                testing_error = (mean_squared_error(y_test.values, y_test_predicted))**0.5/y_test.mean()\n",
    "\n",
    "                saved_result_df = pd.concat([pd.DataFrame(y_train.values), pd.DataFrame(y_train_predicted),\n",
    "                                             pd.DataFrame(y_test.values), pd.DataFrame(y_test_predicted)], axis = 1)\n",
    "                saved_result_df.columns = ['y_train','_y_train_predicted','y_test','y_test_predicted']\n",
    "                saved_result_df.to_csv(f'results/{self.version}/y_train_and_y_test_{self.building_type}.csv', index = None)\n",
    "\n",
    "                error_df[building_type] = [training_error.values[0], testing_error.values[0]]\n",
    "                error_df.index = ['training_error','testing_error']\n",
    "                error_df.to_csv(f'results/{self.version}/error_{self.building_type}.csv')\n",
    "\n",
    "                feature_importance_df_final.to_csv(f'results/{self.version}/feature_importance_{self.building_type}.csv')\n",
    "\n",
    "                # plots module is added here\n",
    "\n",
    "        elif train_or_load_model == 'load':\n",
    "            print(f'Loading Model: {building_type} ...')\n",
    "            self.model = pickle.load(open(f'models/{version}/{building_type}.sav', 'rb'))\n",
    "            print('Loading Model Completed!')\n",
    "        else:\n",
    "            print (\"Error! Enter either 'train' or 'load' for train_or_load_model\")\n",
    "    # to be worked on this def\n",
    "    def make_predictions(self):\n",
    "        version = self.version\n",
    "        building_type = self.building_type\n",
    "        self.output_test = self.model.predict(self.inputs_test.iloc[:,2:])\n",
    "        pd.DataFrame(self.output_test, columns = ['output_test']).to_csv(f'results/{version}/predictions_{building_type}.csv', index = None)\n",
    "        print('Make and saving predictions...')\n",
    "\n",
    "    def QOI_calculators(self):\n",
    "        print('Calculating and saving QOIs...')\n",
    "        temp_dask_df = pd.DataFrame(self.y_test_predicted, columns = ['total_site_electricity_kwh'])\n",
    "        temp_dask_df ['Geometry_Building_Type_RECS'] = self.building_type\n",
    "        temp_dask_df ['time'] = np.repeat(pd.date_range(\"01-01-2016 00:00\", \"12-30-2016 23:00\", freq=\"60min\"), int(len(temp_dask_df)/(365*24)))\n",
    "        temp_dask_df ['Building'] = 0\n",
    "        temp_dask_df = dd.from_pandas(pd.DataFrame(temp_dask_df), npartitions = 10)\n",
    "\n",
    "        QOI_model = QuantityOfInterest(temp_dask_df)\n",
    "        QOI_model.update_dask_df(temp_dask_df)\n",
    "        temp_dict = QOI_model.get_all_qoi(building_type=self.building_type)\n",
    "        self.QOI_calculation_results = pd.DataFrame.from_dict(temp_dict, orient='index', columns = ['total_site_electricity_kwh_or_time_of_day'])\n",
    "        self.QOI_calculation_results.to_csv(f\"results/{self.version}/QOI_calculations_{self.building_type}_validation_predict.csv\")\n",
    "\n",
    "        print('Calculating and saving QOIs...')\n",
    "        temp_dask_df = pd.DataFrame(self.y_test, columns = ['total_site_electricity_kwh'])\n",
    "        temp_dask_df ['Geometry_Building_Type_RECS'] = self.building_type\n",
    "        temp_dask_df ['time'] = np.repeat(pd.date_range(\"01-01-2016 00:00\", \"12-30-2016 23:00\", freq=\"60min\"), int(len(temp_dask_df)/(365*24)))\n",
    "        temp_dask_df ['Building'] = 0\n",
    "        temp_dask_df = dd.from_pandas(pd.DataFrame(temp_dask_df), npartitions = 10)\n",
    "\n",
    "        QOI_model = QuantityOfInterest(temp_dask_df)\n",
    "        QOI_model.update_dask_df(temp_dask_df)\n",
    "        temp_dict = QOI_model.get_all_qoi(building_type=self.building_type)\n",
    "        self.QOI_calculation_results = pd.DataFrame.from_dict(temp_dict, orient='index', columns = ['total_site_electricity_kwh_or_time_of_day'])\n",
    "        self.QOI_calculation_results.to_csv(f\"results/{self.version}/QOI_calculations_{self.building_type}_validation_truth.csv\")\n",
    "\n",
    "    def whole_process_only_training(self):\n",
    "        self.create_folder_structure()\n",
    "        self.inputs_generator(train_or_test = 'train')\n",
    "        self.output_generator()\n",
    "        self.get_models(train_or_load_model = 'train')\n",
    "        self.QOI_calculators()\n",
    "\n",
    "    def whole_process_training_and_testing(self):\n",
    "        self.create_folder_structure()\n",
    "        self.inputs_generator(train_or_test = 'train')\n",
    "        self.output_generator()\n",
    "        self.get_models(train_or_load_model = 'train')\n",
    "        self.inputs_generator(train_or_test = 'test')\n",
    "        self.make_predictions()\n",
    "        self.QOI_calculators()\n",
    "\n",
    "    def whole_process_only_testing(self):\n",
    "        self.create_folder_structure()\n",
    "        self.get_models(train_or_load_model = 'load')\n",
    "        self.inputs_generator(train_or_test = 'test')\n",
    "        self.make_predictions()\n",
    "        self.QOI_calculators()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: primary_school\n",
      "Creating folder structure...\n",
      "Generating inputs for training...\n",
      "Generating output...\n",
      "Training Model: primary_school ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [52, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4cfdd10d86f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m                                   raw_more_building_characteristics_for_prediction = 'data/com_reg1_02_01_01_2016/com_reg1_02_01_01_2016_results_filter.csv')\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mtest_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhole_process_only_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-5848c4702b4a>\u001b[0m in \u001b[0;36mwhole_process_only_training\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_or_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_or_load_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQOI_calculators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5848c4702b4a>\u001b[0m in \u001b[0;36mget_models\u001b[1;34m(self, train_or_load_model)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;31m# Here we train the annual building energy consumption model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mannual_load_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0mannual_load_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m//\u001b[0m\u001b[1;36m8760\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_train_annual_total\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[0mfeature_importance_df_annual_energy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\EULP-uncertainty-quantification-qoi_bugfix\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             )\n\u001b[1;32m--> 303\u001b[1;33m         X, y = self._validate_data(X, y, multi_output=True,\n\u001b[0m\u001b[0;32m    304\u001b[0m                                    accept_sparse=\"csc\", dtype=DTYPE)\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\EULP-uncertainty-quantification-qoi_bugfix\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\EULP-uncertainty-quantification-qoi_bugfix\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\EULP-uncertainty-quantification-qoi_bugfix\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 812\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\EULP-uncertainty-quantification-qoi_bugfix\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [52, 4]"
     ]
    }
   ],
   "source": [
    "# from ComStock_Surrogate import ComStock_Surrogate as ComStock_Surrogate\n",
    "\n",
    "# building_type_list = ['full_service_restaurant',\n",
    "#                         'small_office',\n",
    "#                         'warehouse',\n",
    "#                         'retail',\n",
    "#                         'outpatient',\n",
    "#                         'strip_mall',\n",
    "#                         'large_office',\n",
    "#                         'small_hotel',\n",
    "#                         'quick_service_restaurant',\n",
    "#                         'medium_office',\n",
    "#                         'primary_school',\n",
    "#                         'hospital',\n",
    "#                         'secondary_school',\n",
    "#                         'large_hotel',]\n",
    "\n",
    "building_type_list = [\n",
    "                        # 'secondary_school',\n",
    "                        # 'large_office',\n",
    "                        # 'hospital',\n",
    "                        # 'large_hotel',\n",
    "                        # 'medium_office',\n",
    "                        # 'small_hotel',\n",
    "                        # 'quick_service_restaurant',\n",
    "                        'primary_school',\n",
    "                        ]\n",
    "\n",
    "for building_type in building_type_list:\n",
    "    print(f'Processing: {building_type}')\n",
    "\n",
    "    test_run = ComStock_Surrogate(version = 'com_reg1_02_01_01_2016',\n",
    "                                  result_csv = 'data/com_reg1_02_01_01_2016/results_up00_com_reg1_02_01_01_2016_filter.parquet',\n",
    "                                  raw_more_building_characteristics = 'data/com_reg1_02_01_01_2016/com_reg1_02_01_01_2016_results_filter.csv',\n",
    "                                  read_data_from_S3 = False, number_of_trees = 10,\n",
    "                                  building_type = building_type, validation = False,\n",
    "                                  result_csv_for_prediction = 'data/com_reg1_02_01_01_2016/results_up00_com_reg1_02_01_01_2016_filter.parquet',\n",
    "                                  raw_more_building_characteristics_for_prediction = 'data/com_reg1_02_01_01_2016/com_reg1_02_01_01_2016_results_filter.csv')\n",
    "\n",
    "    test_run.whole_process_only_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35040"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_run.output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ComStock_Surrogate' object has no attribute 'input_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-867f45a26f0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'ComStock_Surrogate' object has no attribute 'input_train'"
     ]
    }
   ],
   "source": [
    "len(test_run.input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
